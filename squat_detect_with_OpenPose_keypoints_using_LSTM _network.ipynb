{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " squat-detect-latest-keypoints-to-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pappuyadav/Squat-Detection-Using-VGG16-and-LSTM/blob/master/squat_detect_with_OpenPose_keypoints_using_LSTM%20_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38L6tanrnrB",
        "colab_type": "text"
      },
      "source": [
        "# Pose Detection with OpenPose\n",
        "\n",
        "This notebook uses an open source project [CMU-Perceptual-Computing-Lab/openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose.git) to detect/track multi person poses on a given youtube video.\n",
        "\n",
        "For other deep-learning Colab notebooks, visit [tugstugi/dl-colab-notebooks](https://github.com/tugstugi/dl-colab-notebooks).\n",
        "\n",
        "\n",
        "## Install OpenPose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOdkDhb6ga6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  !pip install -q youtube-dl\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`\n",
        "  \n",
        "from IPython.display import YouTubeVideo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiv14oMBna2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmY4kjdjZH-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download example video file from youtube\n",
        "!youtube-dl -f 'bestvideo[ext=mp4]' --output \"andrew_jiang.%(ext)s\" https://www.youtube.com/watch?v=2vigWc0YMKE&feature=youtu.be"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKrIZ32IjsZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate two directories. \n",
        "!mkdir keypoints\n",
        "!mkdir frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIHoRTsLjd4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generating image frames for each video files in avi format. Here example video file name is \"sq8.avi\". \n",
        "#Upload your test video file (in avi format) in the home folder then replace 'output_sq4.mp4' with your video file\n",
        "import cv2\n",
        "vidcap = cv2.VideoCapture('/content/sq8.avi')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "  cv2.imwrite(\"/content/frames/sq8_frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
        "  success,image = vidcap.read()\n",
        "  print ('Read a new frame: ', success)\n",
        "  count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te_aX-mnITtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this section to create pose detect. Make sure to use right video files in .avi format  \n",
        "!cd openpose && ./build/examples/openpose/openpose.bin --video ../andrew_jiang.mp4 --write_json ./output/ --display 0  --write_video ../openpose_ajiang.avi\n",
        "!ffmpeg -y -loglevel info -i /content/openpose_ajiang.avi /content/openpose_ajiang.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10RgNB-pe_xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Please make two directories to put training image frames and test image frames\n",
        "#First half of training image frames consist of non-squat and remaining half consists of squat frames\n",
        "#701 training images frames are used.Keypoints corresponding to these are used for training and validation\n",
        "#267 frames blong to squat and remaining 434 frames belong to no-squat\n",
        "#Please use your keypoints corresponding to squat frames in /content/keypoints/train_squat and corresponding to no-squat in /content/keypoints/train_nosquat\n",
        "\n",
        "!mkdir /content/keypoints/train_squat\n",
        "!mkdir /content/keypoints/train_nosquat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1oGxJh7fQs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten,GlobalAveragePooling2D,Input,LSTM,Embedding, TimeDistributed\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from scipy import ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "import cv2\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "\n",
        "\n",
        "Name=\"Squat_Detection{}\".format(int(time.time()))\n",
        "\n",
        "tensorboard=TensorBoard(log_dir='logs/{}'.format(Name))\n",
        "\n",
        "#Uisng 2D Keypoints generated from Openpose\n",
        "# Loading 2D keypoints from JSON files\n",
        "def read_josn(filepath):\n",
        "  with open(filepath,'r')as f:\n",
        "    data=json.load(f)\n",
        "  return(data)\n",
        "\n",
        "trainsquat_data=[]\n",
        "for file in os.listdir('/content/keypoints/train_squat'):\n",
        "  full_path='/content/keypoints/train_squat/'+ str(file)\n",
        "  aa=read_josn(full_path)\n",
        "  df=pd.DataFrame(aa)\n",
        "  df=df['people'][0]['pose_keypoints_2d']\n",
        "  trainsquat_data.append(df)\n",
        "  #print(df)\n",
        "\n",
        "train_nosquat_data=[]\n",
        "for file in os.listdir('/content/keypoints/train_nosquat'):\n",
        "  full_path='/content/keypoints/train_nosquat/'+ str(file)\n",
        "  !rmdir /content/keypoints/train_nosquat/.ipynb_checkpoints\n",
        "  aa=read_josn(full_path)\n",
        "  df=pd.DataFrame(aa)\n",
        "  df=df['people'][0]['pose_keypoints_2d']\n",
        "  train_nosquat_data.append(df)\n",
        "  #print(df)\n",
        "squat_train=[]\n",
        "no_squat_train=[]\n",
        "squat_label=[]\n",
        "no_squat_label=[]\n",
        "labels=[]\n",
        "#creating labels for squat and no_squat\n",
        "for i in range(len(trainsquat_data)):\n",
        "  squat_label.append(1)\n",
        "#print(squat_label)\n",
        "\n",
        "for i in range(len(train_nosquat_data)):\n",
        "  no_squat_label.append(0)\n",
        "#print(no_squat_label)\n",
        "\n",
        "#combining the lables in one array\n",
        "squat_label.extend(no_squat_label)\n",
        "labels=np.array(squat_label)\n",
        "\n",
        "#Creating training array\n",
        "trainsquat_data.extend(train_nosquat_data)\n",
        "squat_train=np.array(trainsquat_data)\n",
        "\n",
        "#Defining number of classes\n",
        "num_classes=2\n",
        "num_samples=squat_train.shape[0]\n",
        "labels=np.ones(num_samples,dtype='int64')\n",
        "labels[0:267]=1  # Of the 701 frames,first 267 frames belong to squat i.e. class'1'\n",
        "labels[267:701]=0  # remaining 434 frames belong to no_squat  i.e. class '0'\n",
        "names=['squat','no_squat']\n",
        "#convert class labels to one-hot encoding\n",
        "Y = np_utils.to_categorical(labels, num_classes)\n",
        "#Shuffle the training data set\n",
        "xtrain,ytrain=shuffle(squat_train,Y,random_state=2)\n",
        "#Splitting data for validation set\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(squat_train, labels, test_size=0.20, shuffle= True)\n",
        "\n",
        "#Reshaping the data into 1 rows and 75 columns for 560 data values of training data and 141 data values of validation data\n",
        "x_train=x_train.reshape(len(x_train),1,75)\n",
        "x_valid=x_valid.reshape(len(x_valid),1,75)\n",
        "#y_train=y_train.reshape(1,len(y_train))\n",
        "#y_valid=y_valid.reshape(1,len(y_valid))\n",
        "\n",
        "#The Dataset is ready to be fed into LSTM network\n",
        "# We will now build and train LSTM architecture and then make predictions for squat and no_squat\n",
        "\n",
        "X_train=x_train\n",
        "Y_train=y_train\n",
        "X_test=x_valid\n",
        "Y_test=y_valid\n",
        "\n",
        "# Truncating and padding input sequences of data after reshaping them \n",
        "timestep=1 \n",
        "X_train = sequence.pad_sequences(X_train, maxlen=timestep)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=timestep) \n",
        "\n",
        "\n",
        "#Building the LSTM architecture\n",
        "size=50\n",
        "embedding_vecor_length =8\n",
        "lstm_model = Sequential()\n",
        "#lstm_model.add(Embedding(size,embedding_vecor_length, input_length=timestep))\n",
        "lstm_model.add(LSTM(128,input_shape=(X_train.shape[1:]),activation='relu',return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(128, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(32,activation='relu'))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(num_classes, activation='sigmoid'))\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(lstm_model.summary())\n",
        "hist=lstm_model.fit(X_train, Y_train, epochs=500, batch_size=100,verbose=1, validation_data=(X_test,Y_test), callbacks=[tensorboard])\n",
        "\n",
        "#print('Training time: %s' % (t - time.time()))\n",
        "(loss, accuracy) = lstm_model.evaluate(X_test,Y_test, batch_size=100, verbose=1)\n",
        "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\n",
        "\n",
        "#Evaluating the LSTM model\n",
        "train_acc = lstm_model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_acc = lstm_model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "temp=lstm_model.predict(X_test)\n",
        "temp1=lstm_model.predict_classes(X_test)\n",
        "\n",
        "# reduce to 1d array\n",
        "yhat_probs = temp[:, 0]\n",
        "yhat_classes = temp1[:, 0]\n",
        "\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(Y_test, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(Y_test, yhat_classes)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(Y_test, yhat_classes)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(Y_test, yhat_classes)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "# visualizing losses and accuracy\n",
        "train_loss=hist.history['loss']\n",
        "val_loss=hist.history['val_loss']\n",
        "train_acc=hist.history['acc']\n",
        "val_acc=hist.history['val_acc']\n",
        "xc=range(500)\n",
        "\n",
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])\n",
        "\n",
        "\n",
        "\n",
        "#Now saving the pretrained weights\n",
        "# serialize model to JSON\n",
        "model_json = lstm_model.to_json()\n",
        "with open(\"trained_data_squats.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "lstm_model.save_weights(\"trained_data_squats.h5\")\n",
        "print(\"Saved trained_data to disk\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WtS8qT8XNGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Zipping header and json files from trained model and downloading them\n",
        "\n",
        "from google.colab import files\n",
        "!zip -r trained_data_header.zip /content/trained_data_squats.h5\n",
        "!zip -r trained_data_json.zip /content/trained_data_squats.json\n",
        "files.download('trained_data_header.zip')\n",
        "files.download('trained_data_json.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}